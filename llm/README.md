# Sistema de Embeddings y Base de Datos Vectorial

Este sistema implementa bÃºsqueda semÃ¡ntica usando embeddings locales (BAAI/bge-base-en-v1.5) y Pinecone como base de datos vectorial.

## ğŸ—ï¸ Arquitectura

```
llm/
â”œâ”€â”€ __init__.py              # Paquete Python
â”œâ”€â”€ requirements.txt         # Dependencias
â”œâ”€â”€ pinecone_client.py      # Cliente Pinecone
â”œâ”€â”€ embedder.py             # Modelo de embeddings local
â”œâ”€â”€ vector_db.py            # Sistema integrado
â”œâ”€â”€ ingest.py               # Script de ingesta de datos
â”œâ”€â”€ query.py                # Script de consultas
â”œâ”€â”€ generate_keywords.py    # Script para generar palabras clave
â””â”€â”€ README.md               # Esta documentaciÃ³n
```

## ğŸš€ ConfiguraciÃ³n

### 1. Variables de Entorno

AsegÃºrate de tener en tu archivo `.env`:
```bash
PINECONE_API_KEY=tu_api_key_de_pinecone
```

### 2. Instalar Dependencias

```bash
pip install -r llm/requirements.txt
```

## ğŸ“Š Especificaciones TÃ©cnicas

- **Modelo de Embeddings**: BAAI/bge-base-en-v1.5
- **Dimensiones**: 768
- **Idiomas**: BilingÃ¼e (EspaÃ±ol/InglÃ©s)
- **Base de Datos Vectorial**: Pinecone
- **MÃ©trica de Similitud**: Coseno
- **OptimizaciÃ³n**: Float16 para GPU, CPU nativo
- **Campo para Embeddings**: `palabras_clave_ia` (preferido) o `resumen_ia`

## ğŸ”§ Uso

### 1. Generar Palabras Clave (Opcional pero Recomendado)

```bash
# Generar palabras clave de resÃºmenes IA existentes
python -m llm.generate_keywords
```

Este script:
- Extrae palabras clave de resÃºmenes IA usando DeepSeek
- Optimiza el texto para embeddings (50-100 palabras vs 300+ del resumen)
- Mejora la eficiencia y precisiÃ³n de bÃºsquedas

### 2. Poblar la Base Vectorial

```bash
# Desde la raÃ­z del proyecto
python -m llm.ingest
```

Este script:
- Obtiene todos los lugares con resÃºmenes IA de la base de datos
- Usa `palabras_clave_ia` si estÃ¡ disponible, sino `resumen_ia`
- Genera embeddings para cada texto
- Los inserta en Pinecone en lotes de 100

### 3. Hacer Consultas SemÃ¡nticas

```bash
# Modo interactivo
python -m llm.query
```

Comandos disponibles:
- `buscar <consulta>` - Buscar lugares similares
- `tipo <tipo> <consulta>` - Buscar por tipo especÃ­fico
- `stats` - Ver estadÃ­sticas del Ã­ndice
- `quit` - Salir

### 4. Uso ProgramÃ¡tico

```python
from llm.vector_db import get_vector_db

# Obtener sistema de base de datos vectorial
vector_db = get_vector_db()

# Buscar lugares similares
results = vector_db.search_similar("hotel con piscina", top_k=5)

# Buscar por tipo especÃ­fico
hotels = vector_db.search_by_tipo("hotel con spa", "hotel", top_k=3)
```

## ğŸ“ˆ Rendimiento

### Optimizaciones Implementadas

1. **Palabras Clave Optimizadas**: 50-100 palabras vs 300+ del resumen
2. **Batch Processing**: Procesamiento en lotes de 100 vectores
3. **GPU Optimization**: Float16 para GPU automÃ¡tico
4. **Memory Management**: NormalizaciÃ³n de embeddings
5. **Error Handling**: Manejo robusto de errores

### MÃ©tricas Esperadas

- **Velocidad de Embedding**: ~100-200 textos/segundo (CPU)
- **PrecisiÃ³n**: Alta similitud semÃ¡ntica con palabras clave
- **Escalabilidad**: Hasta millones de vectores
- **Eficiencia**: 3x mÃ¡s rÃ¡pido con palabras clave vs resÃºmenes completos

## ğŸ” Ejemplos de Uso

### Consultas TÃ­picas

```python
# Hoteles con amenidades especÃ­ficas
"hotel con gimnasio y spa"
"hotel para familias con niÃ±os"

# Restaurantes por tipo de comida
"restaurante romÃ¡ntico italiano"
"comida rÃ¡pida vegetariana"

# Actividades de entretenimiento
"parque de diversiones familiar"
"museo de arte moderno"

# BÃºsquedas por ubicaciÃ³n
"restaurante cerca del centro"
"hotel con vista al mar"
```

### Filtros por Tipo

```python
# Solo hoteles
vector_db.search_by_tipo("con piscina", "hotel")

# Solo restaurantes
vector_db.search_by_tipo("romÃ¡ntico", "restaurant")

# Solo bares
vector_db.search_by_tipo("mÃºsica en vivo", "bar")
```

## ğŸ› ï¸ Mantenimiento

### Verificar Estado del Sistema

```python
from llm.vector_db import get_vector_db

vector_db = get_vector_db()
stats = vector_db.get_index_stats()
print(f"Total de vectores: {stats.get('total_vector_count')}")
```

### Recrear Ãndice

```python
# Eliminar Ã­ndice existente
vector_db.delete_index()

# Crear nuevo Ã­ndice
vector_db.create_index()

# Repoblar datos
python -m llm.ingest
```

### Actualizar Palabras Clave

```bash
# Generar palabras clave para nuevos lugares
python -m llm.generate_keywords
```

## ğŸ› Troubleshooting

### Problemas Comunes

1. **Error de API Key**: Verificar `PINECONE_API_KEY` en `.env`
2. **Modelo no carga**: Verificar conexiÃ³n a internet para descargar modelo
3. **Memoria insuficiente**: Reducir batch_size en ingest.py
4. **Dimensiones incorrectas**: Verificar que el modelo sea BAAI/bge-base-en-v1.5
5. **Palabras clave vacÃ­as**: Ejecutar `python -m llm.generate_keywords`

### Logs

El sistema genera logs detallados:
- Nivel INFO: Operaciones principales
- Nivel WARNING: Problemas no crÃ­ticos
- Nivel ERROR: Errores que requieren atenciÃ³n

## ğŸ”„ IntegraciÃ³n con ChatBot

Para integrar con el sistema de chatbot existente:

```python
from llm.vector_db import get_vector_db

def buscar_lugares_semantico(query: str, tipo: str = None):
    vector_db = get_vector_db()
    
    if tipo:
        results = vector_db.search_by_tipo(query, tipo, top_k=5)
    else:
        results = vector_db.search_similar(query, top_k=5)
    
    return results
```

## ğŸ“š Referencias

- [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)
- [Pinecone Documentation](https://docs.pinecone.io/)
- [Sentence Transformers](https://www.sbert.net/) 